{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8efb3-dfcb-4a0f-bde1-4f0c18187566",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SpeechRecognition\n",
    "!pip install matplotlib\n",
    "!pip install librosa\n",
    "!pip install pandas\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c5e7b-c9a5-4884-9de8-8fcf7431a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54263fe6-2d8f-4fc4-9e3e-a4fbc52dfd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "recognizer = sr.Recognizer()\n",
    "# input a file_path to a .wav file\n",
    "# returns the transcribed audio as a string\n",
    "# we can use BERT like in the homework to then tokenize/make into array and analyze it\n",
    "def getVectorOfWords(file_path):\n",
    "    with sr.AudioFile(file_path) as source:\n",
    "        audio = recognizer.record(source)\n",
    "    try:\n",
    "        # print(\"Transcription:\", recognizer.recognize_google(audio))\n",
    "        return \"\" + recognizer.recognize_google(audio)\n",
    "    except sr.UnknownValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b80b3-cfc9-4e91-b3c6-c9f40bec6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes all files from images folder so subsequent runs don't have weird overlaps\n",
    "def clearImagesFolder():\n",
    "    print(\"Deleting all data from images folder\")\n",
    "    directory = os.getcwd() + \"/images\"\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):  # topdown=False to delete files before dirs\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            if os.path.isfile(file_path) and file_name.endswith('.png'):\n",
    "                os.remove(file_path)\n",
    "                # print(\"\" + file_path + \" has been removed successfully\")\n",
    "    print(\"All images removed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084ca90-8df5-4dd2-b9e9-8e46b31603f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "# input a file_path to a .wav file\n",
    "# returns a png of the spectogram and a filepath to it\n",
    "def getSpectogram(file_path, emotion_label):\n",
    "    y, sr = librosa.load(file_path, sr=None) # load in the audio file and preserve its sample rate (replace with 16,000 if needed)\n",
    "    \n",
    "    # Compute the spectrogram\n",
    "    D = librosa.stft(y)                        # Short-Time Fourier Transform\n",
    "    S_db = librosa.amplitude_to_db(abs(D), ref=np.max)  # Convert to decibel scale\n",
    "\n",
    "    # Plot and save the spectrogram\n",
    "    fig = plt.figure(figsize=(6, 6))                # Set the figure size -- > num pixels will be 100 times this\n",
    "    # can change the cmap to \"viridis\" or \"plasma\" for different color themes\n",
    "    librosa.display.specshow(S_db, sr=sr, x_axis=\"time\", y_axis=\"log\", cmap=\"magma\")  # Log frequency scale to mimic human audio perception\n",
    "\n",
    "    # TODO: at first try hiding as many extra features as possible and compare to when they're included\n",
    "    # plt.colorbar(format=\"%+2.0f dB\")           # Add a colorbar\n",
    "    # plt.title(\"Spectrogram\")\n",
    "    # plt.xlabel(\"Time (s)\")\n",
    "    # plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the spectrogram as an image file\n",
    "    processed_path = (file_path.split(\"/\")[-1]).split(\".\")[0]\n",
    "    output_image_path = f\"./images/{emotion_label}/{processed_path}.png\"  # TODO: figure out naming conventions for the file -- either use path or just have a counter that we pass in\n",
    "    plt.savefig(output_image_path, dpi=300)    # Save as PNG with high resolution\n",
    "    plt.close()                                # Close the figure to free memory\n",
    "    \n",
    "    return output_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d362f-4b74-42a0-a66b-e489d18e1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargetEmotionFromCSV(audio_file_name):\n",
    "    # parse audio_file_name to get distinguishing file info for CSV lookup\n",
    "    dialogueID, utteranceID = (audio_file_name.split(\".wav\")[0]).split('_')\n",
    "    dialogueID, utteranceID = int(dialogueID[3:]), int(utteranceID[3:])\n",
    "    csv = pd.read_csv('./train_sent_emo.csv')\n",
    "    # Filter the row(s) that satisfy both conditions\n",
    "    condition1 = (csv['Dialogue_ID'] == dialogueID)  # First column matches 'dialogueID'\n",
    "    condition2 = (csv['Utterance_ID'] == utteranceID)  # Second column matches 'utteranceID'\n",
    "    filtered_rows = csv[condition1 & condition2]\n",
    "    return filtered_rows['Emotion'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844d128-e096-4b4f-8b68-ce10bcd26763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_audio_files(directory=\"./train_splits_wav\"):\n",
    "    clearImagesFolder() # deletes everything from the image folder\n",
    "    data = []\n",
    "    \n",
    "    # Traverse and process .wav files\n",
    "    print(\"Starting audio file traversal\")\n",
    "    iterCount = 0\n",
    "    for file_name in os.listdir(directory):\n",
    "        # limit the number of loops so this doesn't take THAT long\n",
    "        if iterCount >= 200:\n",
    "            break\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_name.endswith('.wav'):\n",
    "            transcription = getVectorOfWords(file_path)\n",
    "            # filter out the audio files that can't get a clear transcription\n",
    "            if not transcription:\n",
    "                continue\n",
    "            emotion = getTargetEmotionFromCSV(file_name)\n",
    "            image_path = getSpectogram(file_path, emotion)\n",
    "            data.append({\"Transcription\": transcription, \"Spectogram\": image_path, \"Emotion\": emotion})\n",
    "        iterCount += 1\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Finished creating dataframe and traversing audio files\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbccad4-3ff2-49a8-b96a-4cca8c6922ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = traverse_audio_files()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d72e0-dcba-480e-9970-5e0ddbb9d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start NN here\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import keras.callbacks\n",
    "# code following homework model\n",
    "def preprocessingNN(image_size=(600, 600), batch_size=32):\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        \"images\",\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=1337,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical' # one-hot encodes\n",
    "    )\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        \"images\",\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=1337,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical' # one-hot encodes\n",
    "    )\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165cdcfc-6e28-4f81-95f2-9c0561c19df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAndRunNN(train_ds, val_ds):\n",
    "    # Network structure from https://keras.io/examples/vision/mnist_convnet/\n",
    "    input_shape = (600,600,3)\n",
    "    num_classes = 7\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=input_shape),\n",
    "            layers.Rescaling(scale=1./255, offset=0.0),\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    # fit the model\n",
    "    # batch_size = 128\n",
    "    epochs = 15\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)  # Quit after 3 rounds of no validation loss improvement\n",
    "    ]\n",
    "    \n",
    "    model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks) # Validation data instead of fraction\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd070384-7066-419c-ba84-799cde0d380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: add some more preprocessing to even out the categories of data\n",
    "def runNN():\n",
    "    train_ds, val_ds = preprocessingNN()\n",
    "    model = fitAndRunNN(train_ds, val_ds)\n",
    "    return model\n",
    "print(sum(df['Emotion'] == \"neutral\") / len(df['Emotion']) * 100, \"percent of the dataset is labeled 'neutral'.\")\n",
    "runNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53eb389-60a9-466e-8d33-85cd2b29ceef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
