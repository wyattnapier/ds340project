{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a8efb3-dfcb-4a0f-bde1-4f0c18187566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (3.11.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from SpeechRecognition) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from SpeechRecognition) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests>=2.26.0->SpeechRecognition) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests>=2.26.0->SpeechRecognition) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n",
      "Requirement already satisfied: matplotlib in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: librosa in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: packaging in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
      "Requirement already satisfied: pandas in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: tensorflow in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (5.29.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: rich in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: transformers in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: evaluate in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: seqeval in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: filelock in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from seqeval) (1.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/my_conda_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition\n",
    "!pip install matplotlib\n",
    "!pip install librosa\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install transformers datasets evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3c5e7b-c9a5-4884-9de8-8fcf7431a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54263fe6-2d8f-4fc4-9e3e-a4fbc52dfd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "recognizer = sr.Recognizer()\n",
    "# input a file_path to a .wav file\n",
    "# returns the transcribed audio as a string\n",
    "# we can use BERT like in the homework to then tokenize/make into array and analyze it\n",
    "def getVectorOfWords(file_path):\n",
    "    with sr.AudioFile(file_path) as source:\n",
    "        audio = recognizer.record(source)\n",
    "    try:\n",
    "        # print(\"Transcription:\", recognizer.recognize_google(audio))\n",
    "        return \"\" + recognizer.recognize_google(audio)\n",
    "    except sr.UnknownValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "650b80b3-cfc9-4e91-b3c6-c9f40bec6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes all files from images folder so subsequent runs don't have weird overlaps\n",
    "def clearImagesFolder():\n",
    "    print(\"Deleting all data from images folder\")\n",
    "    directory = os.getcwd() + \"/images\"\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):  # topdown=False to delete files before dirs\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            if os.path.isfile(file_path) and file_name.endswith('.png'):\n",
    "                os.remove(file_path)\n",
    "                # print(\"\" + file_path + \" has been removed successfully\")\n",
    "    print(\"All images removed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d084ca90-8df5-4dd2-b9e9-8e46b31603f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "# input a file_path to a .wav file\n",
    "# returns a png of the spectogram and a filepath to it\n",
    "def getSpectogram(file_path, emotion_label):\n",
    "    y, sr = librosa.load(file_path, sr=None) # load in the audio file and preserve its sample rate (replace with 16,000 if needed)\n",
    "    \n",
    "    # Compute the spectrogram\n",
    "    D = librosa.stft(y)                        # Short-Time Fourier Transform\n",
    "    S_db = librosa.amplitude_to_db(abs(D), ref=np.max)  # Convert to decibel scale\n",
    "\n",
    "    # Plot and save the spectrogram\n",
    "    fig = plt.figure(figsize=(6, 6))                # Set the figure size -- > num pixels will be 100 times this\n",
    "    # can change the cmap to \"viridis\" or \"plasma\" for different color themes\n",
    "    librosa.display.specshow(S_db, sr=sr, x_axis=\"time\", y_axis=\"log\", cmap=\"magma\")  # Log frequency scale to mimic human audio perception\n",
    "\n",
    "    # TODO: at first try hiding as many extra features as possible and compare to when they're included\n",
    "    # plt.colorbar(format=\"%+2.0f dB\")           # Add a colorbar\n",
    "    # plt.title(\"Spectrogram\")\n",
    "    # plt.xlabel(\"Time (s)\")\n",
    "    # plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the spectrogram as an image file\n",
    "    processed_path = (file_path.split(\"/\")[-1]).split(\".\")[0]\n",
    "    output_image_path = f\"./images/{emotion_label}/{processed_path}.png\"  # TODO: figure out naming conventions for the file -- either use path or just have a counter that we pass in\n",
    "    plt.savefig(output_image_path, dpi=300)    # Save as PNG with high resolution\n",
    "    plt.close()                                # Close the figure to free memory\n",
    "    \n",
    "    return output_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866d362f-4b74-42a0-a66b-e489d18e1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargetEmotionFromCSV(audio_file_name):\n",
    "    # parse audio_file_name to get distinguishing file info for CSV lookup\n",
    "    dialogueID, utteranceID = (audio_file_name.split(\".wav\")[0]).split('_')\n",
    "    dialogueID, utteranceID = int(dialogueID[3:]), int(utteranceID[3:])\n",
    "    csv = pd.read_csv('./train_sent_emo.csv')\n",
    "    # Filter the row(s) that satisfy both conditions\n",
    "    condition1 = (csv['Dialogue_ID'] == dialogueID)  # First column matches 'dialogueID'\n",
    "    condition2 = (csv['Utterance_ID'] == utteranceID)  # Second column matches 'utteranceID'\n",
    "    filtered_rows = csv[condition1 & condition2]\n",
    "    return filtered_rows['Emotion'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2844d128-e096-4b4f-8b68-ce10bcd26763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_audio_files(directory=\"./train_splits_wav\"):\n",
    "    # clearImagesFolder() # deletes everything from the image folder\n",
    "    data = []\n",
    "    \n",
    "    # Traverse and process .wav files\n",
    "    print(\"Starting audio file traversal\")\n",
    "    iterCount = 0\n",
    "    for file_name in os.listdir(directory):\n",
    "        # limit the number of loops so this doesn't take THAT long\n",
    "        if iterCount >= 3000:\n",
    "            break\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_name.endswith('.wav'):\n",
    "            transcription = getVectorOfWords(file_path)\n",
    "            # filter out the audio files that can't get a clear transcription\n",
    "            if not transcription:\n",
    "                continue\n",
    "            emotion = getTargetEmotionFromCSV(file_name)\n",
    "            image_path = getSpectogram(file_path, emotion)\n",
    "            data.append({\"Transcription\": transcription, \"Spectogram\": image_path, \"Emotion\": emotion})\n",
    "        iterCount += 1\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Finished creating dataframe and traversing audio files\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbccad4-3ff2-49a8-b96a-4cca8c6922ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = traverse_audio_files()\n",
    "# df.to_csv('data3000.csv', index=False)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a8d72e0-dcba-480e-9970-5e0ddbb9d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start NN here\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import keras.callbacks\n",
    "### code following homework model\n",
    "def preprocessingNN(image_size=(600, 600), batch_size=32):\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        \"images9000\",\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=1337,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical' # one-hot encodes\n",
    "    )\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        \"images9000\",\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=1337,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical' # one-hot encodes\n",
    "    )\n",
    "    return train_ds, val_ds\n",
    "\n",
    "### BALANCED DATASET\n",
    "# from tensorflow.keras.utils import image_dataset_from_directory\n",
    "# import tensorflow as tf\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "# def preprocessingNN(image_size=(600, 600), batch_size=32, max_neutral=1000):\n",
    "#     print(\"cutting down dataset to use just\", max_neutral, \"neutral samples\")\n",
    "#     # Load the full dataset without splitting\n",
    "#     dataset = image_dataset_from_directory(\n",
    "#         \"images9000\",\n",
    "#         image_size=image_size,\n",
    "#         label_mode='categorical',  # One-hot encodes labels\n",
    "#         batch_size=None  # Load as individual samples for easier processing\n",
    "#     )\n",
    "\n",
    "#     # Class names\n",
    "#     class_names = dataset.class_names\n",
    "#     neutral_idx = class_names.index(\"neutral\")\n",
    "    \n",
    "#     # Separate \"neutral\" and \"other\" images\n",
    "#     neutral_images = []\n",
    "#     other_images = []\n",
    "\n",
    "#     for image, label in dataset:\n",
    "#         if tf.argmax(label).numpy() == neutral_idx:  # Check if it's neutral\n",
    "#             neutral_images.append((image, label))\n",
    "#         else:\n",
    "#             other_images.append((image, label))\n",
    "    \n",
    "#     # Limit the number of \"neutral\" images\n",
    "#     np.random.shuffle(neutral_images)\n",
    "#     limited_neutral_images = neutral_images[:max_neutral]\n",
    "\n",
    "#     # Combine limited \"neutral\" and other images\n",
    "#     balanced_dataset = limited_neutral_images + other_images\n",
    "#     np.random.shuffle(balanced_dataset)\n",
    "\n",
    "#     # Convert lists back to TensorFlow dataset\n",
    "#     balanced_images, balanced_labels = zip(*balanced_dataset)\n",
    "#     balanced_dataset = tf.data.Dataset.from_tensor_slices((list(balanced_images), list(balanced_labels)))\n",
    "\n",
    "#     # Split into training and validation datasets\n",
    "#     split_index = int(len(balanced_dataset) * 0.8)\n",
    "#     train_ds = balanced_dataset.take(split_index).batch(batch_size)\n",
    "#     val_ds = balanced_dataset.skip(split_index).batch(batch_size)\n",
    "\n",
    "#     return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "165cdcfc-6e28-4f81-95f2-9c0561c19df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAndRunNN(train_ds, val_ds):\n",
    "    # Network structure from https://keras.io/examples/vision/mnist_convnet/\n",
    "    input_shape = (600,600,3)\n",
    "    num_classes = 7\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=input_shape),\n",
    "            layers.Rescaling(scale=1./255, offset=0.0),\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5), # added drouput to model4\n",
    "            layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    # fit the model\n",
    "    # batch_size = 128\n",
    "    epochs = 15\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)  # Quit after 5 rounds of no validation loss improvement\n",
    "    ]\n",
    "    \n",
    "    model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks) # Validation data instead of fraction\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd070384-7066-419c-ba84-799cde0d380b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.09945184025059 percent of the dataset is labeled 'neutral'.\n",
      "Found 3831 files belonging to 7 classes.\n",
      "Using 3065 files for training.\n",
      "Found 3831 files belonging to 7 classes.\n",
      "Using 766 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">598</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">598</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78400</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78400</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">548,807</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling_3 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m598\u001b[0m, \u001b[38;5;34m598\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m299\u001b[0m, \u001b[38;5;34m299\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m297\u001b[0m, \u001b[38;5;34m297\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78400\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78400\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │       \u001b[38;5;34m548,807\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">642,055</span> (2.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m642,055\u001b[0m (2.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">642,055</span> (2.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m642,055\u001b[0m (2.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 574ms/step - accuracy: 0.5116 - loss: 1.5581 - val_accuracy: 0.4961 - val_loss: 1.5076\n",
      "Epoch 2/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 576ms/step - accuracy: 0.5099 - loss: 1.5082 - val_accuracy: 0.4961 - val_loss: 1.5004\n",
      "Epoch 3/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 575ms/step - accuracy: 0.5074 - loss: 1.4977 - val_accuracy: 0.4961 - val_loss: 1.4976\n",
      "Epoch 4/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 575ms/step - accuracy: 0.5096 - loss: 1.4927 - val_accuracy: 0.4961 - val_loss: 1.5085\n",
      "Epoch 5/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 584ms/step - accuracy: 0.5094 - loss: 1.4904 - val_accuracy: 0.4961 - val_loss: 1.4853\n",
      "Epoch 6/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 583ms/step - accuracy: 0.5054 - loss: 1.4872 - val_accuracy: 0.4961 - val_loss: 1.5001\n",
      "Epoch 7/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 583ms/step - accuracy: 0.5126 - loss: 1.4642 - val_accuracy: 0.4961 - val_loss: 1.4826\n",
      "Epoch 8/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 589ms/step - accuracy: 0.5139 - loss: 1.4550 - val_accuracy: 0.4961 - val_loss: 1.4920\n",
      "Epoch 9/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 597ms/step - accuracy: 0.5087 - loss: 1.4389 - val_accuracy: 0.4935 - val_loss: 1.4805\n",
      "Epoch 10/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 585ms/step - accuracy: 0.5115 - loss: 1.4076 - val_accuracy: 0.4791 - val_loss: 1.5068\n",
      "Epoch 11/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 589ms/step - accuracy: 0.5216 - loss: 1.2997 - val_accuracy: 0.4700 - val_loss: 1.5655\n",
      "Epoch 12/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 592ms/step - accuracy: 0.5882 - loss: 1.1165 - val_accuracy: 0.4360 - val_loss: 1.6621\n",
      "Epoch 13/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 597ms/step - accuracy: 0.6528 - loss: 0.9107 - val_accuracy: 0.3890 - val_loss: 1.7484\n",
      "Epoch 14/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 595ms/step - accuracy: 0.6802 - loss: 0.8388 - val_accuracy: 0.3969 - val_loss: 2.2089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_3, built=True>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: add some more preprocessing to even out the categories of data\n",
    "def runNN():\n",
    "    train_ds, val_ds = preprocessingNN()\n",
    "    model = fitAndRunNN(train_ds, val_ds)\n",
    "    return model\n",
    "df = pd.read_csv('data9000.csv')\n",
    "print(sum(df['Emotion'] == \"neutral\") / len(df['Emotion']) * 100, \"percent of the dataset is labeled 'neutral'.\")\n",
    "runNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42adfbc0-7e0e-4426-8abd-62d9a1b35e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start BERT here\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertForSequenceClassification\n",
    "# below imports are from github repo linked in NLP homework\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import transformers as ppb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32688b79-1b34-484b-b442-cf3d96cb405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelBERT(df):\n",
    "    # Convert emotion labels to integers\n",
    "    batch = df.loc[:, ['Transcription','Emotion']]\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(df['Emotion'])\n",
    "    batch['Emotion'] = labels # replace the emotions with numerical values (should we one-hot encode instead?)\n",
    "    print(\"batch\", batch)\n",
    "    print(\"Emotion (value) counts:\\n\", batch['Emotion'].value_counts()) # ask for count of each value\n",
    "    \n",
    "    # Get the mapping for later interpretation\n",
    "    emotion_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))) # key val pairs of emotion, int\n",
    "    print(emotion_mapping)\n",
    "    \n",
    "    ###\n",
    "    # num_labels = len(label_encoder.classes_) # Number of unique labels\n",
    "    # model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels) # Load pre-trained BERT for classification\n",
    "    ###\n",
    "    \n",
    "    # For DistilBERT:\n",
    "    model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "    # Load pretrained model/tokenizer\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "    return tokenizer, model, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d86a43e-ed3e-457c-8282-914e9ccd4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepData(tokenizer, batch):\n",
    "    tokenized = batch['Transcription'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "    # pad them all to same length\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    print(\"Padded shape:\", np.array(padded).shape)\n",
    "\n",
    "    # create mask to tell BERT to ignore padding\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    print(\"Attention shape:\", attention_mask.shape)\n",
    "    return padded, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56a60057-3b90-446b-bb0a-7768b4d970c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndRunBERT(padded, attention_mask):\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    labels = batch['Emotion']\n",
    "    \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n",
    "\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_clf.fit(train_features, train_labels)\n",
    "    print(\"Score:\", rf_clf.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b3e8e22-f2e7-4d9c-af37-a0d8c6d26363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch                                           Transcription  Emotion\n",
      "0                                                 Mrs M        4\n",
      "1                                     why did you write        4\n",
      "2                                 I heard what you said        4\n",
      "3                                            for a walk        4\n",
      "4     when did they made me head of purchasing thank...        3\n",
      "...                                                 ...      ...\n",
      "2995                                               fine        0\n",
      "2996                                      say something        6\n",
      "2997                       no he's leaving for a better        4\n",
      "2998                                      you stole the        0\n",
      "2999                               we need a porn break        4\n",
      "\n",
      "[3000 rows x 2 columns]\n",
      "Emotion (value) counts:\n",
      " Emotion\n",
      "4    1469\n",
      "3     479\n",
      "0     326\n",
      "6     317\n",
      "5     252\n",
      "2      80\n",
      "1      77\n",
      "Name: count, dtype: int64\n",
      "{'anger': np.int64(0), 'disgust': np.int64(1), 'fear': np.int64(2), 'joy': np.int64(3), 'neutral': np.int64(4), 'sadness': np.int64(5), 'surprise': np.int64(6)}\n",
      "Padded shape: (3000, 46)\n",
      "Attention shape: (3000, 46)\n",
      "Score: 0.5026666666666667\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data3000.csv')\n",
    "tokenizer, model, batch = modelBERT(df)\n",
    "padded, attention_mask = prepData(tokenizer, batch)\n",
    "trainAndRunBERT(padded, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27566a-64e2-4c0e-b3a3-611b57930153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
